{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bieQzzbCUfb5"
   },
   "source": [
    "# **Specific Test V. Physics-Guided ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU1xoQ1yH18f"
   },
   "source": [
    "1. Imports essential libraries for deep learning (torch, torchvision), data handling (numpy, PIL, os, glob), and evaluation (sklearn.metrics).\n",
    "\n",
    "2. Uses torchvision.transforms for image preprocessing.\n",
    "\n",
    "3. Handles datasets with Dataset, DataLoader, and Subset.\n",
    "\n",
    "4. Loads a pretrained model from torchvision.models.\n",
    "\n",
    "5. Evaluates performance using roc_auc_score and roc_curve.\n",
    "\n",
    "6. Manages files and directories with os and glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aO55CAKHmrX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import models\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBGzVPnhIoUW"
   },
   "source": [
    "\n",
    "* Checks for GPU availability and sets device to \"cuda\" if available, otherwise \"cpu\".\n",
    "\n",
    "* Defines dataset paths for training and validation images, categorized into **three classes: \"no\", \"vort\", and \"sphere\".**\n",
    "\n",
    "* Creates a label mapping assigning integer labels: **\"no\" → 0, \"vort\" → 1, \"sphere\" → 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgkdJ5DQHmrZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define dataset paths\n",
    "train_dirs = {\"no\": \"dataset/dataset/train/no/\", \"vort\": \"dataset/dataset/train/vort/\", \"sphere\": \"dataset/dataset/train/sphere/\"}\n",
    "valid_dirs = {\"no\": \"dataset/dataset/val/no/\", \"vort\": \"dataset/dataset/val/vort/\", \"sphere\": \"dataset/dataset/val/sphere/\"}\n",
    "\n",
    "# Label mapping\n",
    "label_map = {\"no\": 0, \"vort\": 1, \"sphere\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QkJ7IuQJaPy"
   },
   "source": [
    "1. **Custom Dataset (LensingDataset)**\n",
    "\n",
    "> * Inherits from torch.utils.data.Dataset.\n",
    ">* Loads .npy image files and their corresponding labels from specified directories.\n",
    ">* Converts images to float32 and ensures a consistent shape (H, W).\n",
    ">* Applies optional transformations during data retrieval.\n",
    "\n",
    "2. **Data Normalization (compute_mean_std)**\n",
    "\n",
    ">* Computes the mean and standard deviation of all .npy images in the training dataset.\n",
    ">* Uses numpy operations to standardize pixel values (/ 255.0).\n",
    "\n",
    "3. **Data Transformations (transforms.Compose)**\n",
    "\n",
    ">* Converts numpy arrays to PIL.Image.\n",
    ">* Applies random augmentations:\n",
    ">>* Affine transformations (rotation ±15°, translation, scaling).\n",
    ">>* Random horizontal flipping.\n",
    ">>* Random resized cropping (target size: 128×128).\n",
    ">>* Gaussian blur (3×3 kernel).\n",
    ">* Converts images to PyTorch tensors and normalizes them using computed mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-J4lC3aHmrZ"
   },
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class LensingDataset(Dataset):\n",
    "    def __init__(self, directories, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label, path in directories.items():\n",
    "            npy_files = glob(os.path.join(path, \"*.npy\"))\n",
    "            if len(npy_files) == 0:\n",
    "                print(f\"Warning: No .npy files found in {path}\")\n",
    "            for img_path in npy_files:\n",
    "                self.data.append(img_path)\n",
    "                self.labels.append(label_map[label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(self.data[idx]).astype(np.float32)  \n",
    "        image = image.squeeze()  # Ensure shape is (H, W)\n",
    "\n",
    "        label = self.labels[idx]  # Scalar class label\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Convert to tensor (C, H, W)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Compute mean and std for normalization\n",
    "def compute_mean_std(dataset_dirs):\n",
    "    all_data = []\n",
    "    for label, path in dataset_dirs.items():\n",
    "        npy_files = glob(os.path.join(path, \"*.npy\"))\n",
    "        for img_path in npy_files:\n",
    "            img = np.load(img_path).astype(np.float32)\n",
    "            all_data.append(img)\n",
    "\n",
    "    all_data = np.stack(all_data)  # Convert list to numpy array\n",
    "    mean = np.mean(all_data) / 255.0\n",
    "    std = np.std(all_data) / 255.0\n",
    "    return mean, std\n",
    "\n",
    "mean, std = compute_mean_std(train_dirs)\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(128, scale=(0.8, 1.2)),  # Resize after cropping\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[mean], std=[std])  # Normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJCg-xgTM09c"
   },
   "source": [
    "1. **Loads datasets:** Creates LensingDataset instances for training and validation data with transformations.\n",
    "\n",
    "2. **Creates a 10% training subset**\n",
    "\n",
    "* Computes subset size (10% of the full training dataset).\n",
    "\n",
    "* Randomly selects indices using torch.randperm().\n",
    "\n",
    "* Constructs a Subset dataset using the selected indices.\n",
    "\n",
    "3. **Initializes DataLoaders**\n",
    "\n",
    "* train_loader: Loads the training subset with batch size 32, shuffled.\n",
    "\n",
    "* valid_loader: Loads the full validation dataset with batch size 32, not shuffled.\n",
    "\n",
    "4. **Computes and prints class distributions**\n",
    "\n",
    "* Uses Counter to count label occurrences in:\n",
    ">* Full training dataset.\n",
    ">* Full validation dataset.\n",
    ">* Training subset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIIRsdTnHmrZ",
    "outputId": "098b4c3c-2b44-4da4-efb0-4f21b1ea06d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Label Counts: Counter({0: 10000, 1: 10000, 2: 10000})\n",
      "Validation Set Label Counts: Counter({0: 2500, 1: 2500, 2: 2500})\n",
      "Training Subset Label Counts: Counter({2: 1003, 1: 999, 0: 998})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load datasets\n",
    "train_dataset = LensingDataset(train_dirs, transform=transform)\n",
    "valid_dataset = LensingDataset(valid_dirs, transform=transform)\n",
    "\n",
    "# Get 10% of the training dataset\n",
    "subset_size = int(0.1 * len(train_dataset))\n",
    "indices = torch.randperm(len(train_dataset))[:subset_size]  \n",
    "train_subset = Subset(train_dataset, indices)\n",
    "\n",
    "# Create DataLoader for the subset\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print label counts\n",
    "train_label_counts = Counter(train_dataset.labels)\n",
    "print(\"Training Set Label Counts:\", train_label_counts)\n",
    "valid_label_counts = Counter(valid_dataset.labels)\n",
    "print(\"Validation Set Label Counts:\", valid_label_counts)\n",
    "\n",
    "# Get labels of the subset samples\n",
    "train_subset_labels = [train_dataset[idx][1] for idx in indices]  \n",
    "train_subset_label_counts = Counter(train_subset_labels)\n",
    "print(\"Training Subset Label Counts:\", train_subset_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYvRrvp5NgUU"
   },
   "source": [
    "1. **Defines a Physics-Informed ResNet Model (LensingResNet)**\n",
    "\n",
    "* Uses ResNet-50 as the backbone, loading pretrained weights (IMAGENET1K_V1).\n",
    "\n",
    "* Modifies the first convolutional layer to accept grayscale images (1-channel).\n",
    "\n",
    "* Removes the original fully connected (FC) layer (fc = nn.Identity()).\n",
    "\n",
    "2. **Implements Two Prediction Heads**\n",
    "\n",
    "* Classification Head (classifier):\n",
    ">* Fully connected (2048 → 512 → num_classes).\n",
    ">* Uses ReLU activation and Dropout (0.5).\n",
    "\n",
    "* Physics-Informed Head (deflection_head):\n",
    "\n",
    ">* Fully connected (2048 → 512 → 2).\n",
    ">* Predicts deflection angles and applies tanh activation to normalize output between -1 and 1.\n",
    "\n",
    "3. **Handles Input Dimensions:** Ensures input has a channel dimension (x.unsqueeze(1)) for single-channel images.\n",
    "\n",
    "4. **Defines a Physics-Based Loss Function (lensing_loss):**\n",
    "* Computes predicted source position (β) using the lens equation:\n",
    "\n",
    "![Screenshot 2025-03-24 225305.png](data:image/png;base64,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)\n",
    "\n",
    "* Uses Smooth L1 Loss (F.smooth_l1_loss) between predicted and actual source positions (β)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEIozzf4Hmra"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the Physics-Informed ResNet Model\n",
    "class LensingResNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(LensingResNet, self).__init__()\n",
    "        self.base_model = models.resnet50(weights=\"IMAGENET1K_V1\")\n",
    "        self.base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  \n",
    "        self.base_model.fc = nn.Identity()  \n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "        # Physics-informed head for deflection angles\n",
    "        self.deflection_head = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)  # Add channel dimension if missing\n",
    "        features = self.base_model(x)\n",
    "        classification_output = self.classifier(features)\n",
    "        deflection_output = self.deflection_head(features)\n",
    "        deflection_output = torch.tanh(deflection_output)  # Normalize between -1 and 1\n",
    "        return classification_output, deflection_output\n",
    "\n",
    "# Physics Loss Function\n",
    "def lensing_loss(deflection_output, theta, beta, D_LS, D_S):\n",
    "    alpha = deflection_output\n",
    "    theta = theta.unsqueeze(1).expand(-1, 2)\n",
    "    beta = beta.unsqueeze(1).expand(-1, 2)\n",
    "    predicted_beta = theta - (D_LS / D_S) * alpha\n",
    "    physics_loss = F.smooth_l1_loss(predicted_beta, beta)\n",
    "    return physics_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VONLRC84O2zk"
   },
   "source": [
    "1. **Model Initialization**: Defines LensingResNet with 3 output classes and moves it to GPU (cuda) if available.\n",
    "\n",
    "2. **Loss Function & Optimizer**\n",
    "* Uses CrossEntropyLoss for classification.\n",
    "\n",
    "* Optimizes with Adam (lr = 0.0001).\n",
    "\n",
    "3. **Physics Parameters**\n",
    "\n",
    "* Sets lens-source (D_LS) and source (D_S) distances to 1.0 and 2.0, respectively.\n",
    "\n",
    "4. **Training Loop (25 epochs)**\n",
    "\n",
    "> For each epoch:\n",
    ">* Sets the model to training mode (model.train()).\n",
    ">* Iterates over mini-batches from train_loader.\n",
    ">* Ensures grayscale images have a channel dimension (images.unsqueeze(1)).\n",
    ">* Computes classification loss (CrossEntropyLoss).\n",
    ">* Computes physics loss (lensing_loss) using a synthetic θ and β.\n",
    ">* Dynamically adjusts physics loss weight using a sigmoid-based schedule.\n",
    ">* Performs backpropagation (total_loss.backward()) and updates model parameters.\n",
    ">* total_loss = class_loss + physics_loss_weight \\ physics_loss\n",
    "\n",
    "5. **Loss Reduction Trend**\n",
    "\n",
    "* Initial loss: 1.1215 (Epoch 1).\n",
    "\n",
    "* Gradual decline with training.\n",
    "\n",
    "* Final loss: 0.6228 (Epoch 25).\n",
    "\n",
    "* Indicates successful learning and convergence of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0Qm32TpHmra",
    "outputId": "9c2649d4-801b-4a21-c51a-619f11d2abed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1.1215\n",
      "Epoch 2/25, Loss: 1.1094\n",
      "Epoch 3/25, Loss: 1.1056\n",
      "Epoch 4/25, Loss: 1.0969\n",
      "Epoch 5/25, Loss: 1.0977\n",
      "Epoch 6/25, Loss: 1.0911\n",
      "Epoch 7/25, Loss: 1.0794\n",
      "Epoch 8/25, Loss: 1.0545\n",
      "Epoch 9/25, Loss: 1.0127\n",
      "Epoch 10/25, Loss: 0.9824\n",
      "Epoch 11/25, Loss: 0.9347\n",
      "Epoch 12/25, Loss: 0.9000\n",
      "Epoch 13/25, Loss: 0.8653\n",
      "Epoch 14/25, Loss: 0.8433\n",
      "Epoch 15/25, Loss: 0.8081\n",
      "Epoch 16/25, Loss: 0.7780\n",
      "Epoch 17/25, Loss: 0.7423\n",
      "Epoch 18/25, Loss: 0.7289\n",
      "Epoch 19/25, Loss: 0.7038\n",
      "Epoch 20/25, Loss: 0.7031\n",
      "Epoch 21/25, Loss: 0.6934\n",
      "Epoch 22/25, Loss: 0.6514\n",
      "Epoch 23/25, Loss: 0.6398\n",
      "Epoch 24/25, Loss: 0.6385\n",
      "Epoch 25/25, Loss: 0.6228\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LensingResNet(num_classes=3).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Physics parameters\n",
    "D_LS, D_S = 1.0, 2.0  # Testing values for lensing distances\n",
    "\n",
    "# Training loop\n",
    "epochs = 250\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        classification_output, deflection_output = model(images)\n",
    "\n",
    "        # Compute classification loss\n",
    "        class_loss = criterion(classification_output, labels)\n",
    "\n",
    "        # Compute physics loss\n",
    "        batch_size = images.size(0)\n",
    "        theta = torch.ones(batch_size, device=device) * 0.05\n",
    "        beta = torch.zeros(batch_size, device=device)\n",
    "        physics_loss = lensing_loss(deflection_output, theta, beta, D_LS, D_S)\n",
    "\n",
    "        # Adaptive physics loss weighting\n",
    "        physics_loss_weight = 0.1 + (0.4 / (1 + torch.exp(-10 * torch.tensor(epoch / epochs - 0.5, device=device))))\n",
    "        total_loss = class_loss + physics_loss_weight * physics_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9n7WFXlRwRq"
   },
   "source": [
    "1. **Model Evaluation Mode**\n",
    "\n",
    "* Sets model.eval() to disable dropout and batch normalization updates.\n",
    "\n",
    "* Uses torch.no_grad() to prevent gradient computation, reducing memory usage.\n",
    "\n",
    "2. **Validation Loop**\n",
    "\n",
    "* Iterates over validation dataset (valid_loader).\n",
    "\n",
    "* Moves images and labels to GPU (device).\n",
    "\n",
    "* Ensures grayscale images have the correct shape (images.unsqueeze(1)).\n",
    "\n",
    "* Performs forward pass to get:\n",
    "\n",
    ">* Classification output (for label prediction).\n",
    ">* Deflection output (for physics-based loss).\n",
    "\n",
    "3. **Loss Computation**\n",
    "\n",
    "* Classification Loss: CrossEntropyLoss on classification_output.\n",
    "\n",
    "* Physics-Informed Loss: lensing_loss using synthetic θ and β.\n",
    "\n",
    "* Total Loss Calculation: total_loss = class_loss + physics_loss_weight \\ physics_loss\n",
    "4. **Final Validation Loss**\n",
    "\n",
    "* Aggregates total validation loss across all batches.\n",
    "\n",
    "* Computes average loss over the dataset.\n",
    "\n",
    "* Final Validation Loss: 0.8567, indicating generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIOd5TcQHmrb",
    "outputId": "e80be2cf-caec-455b-95e7-ae05401cc34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Loss: 0.8567\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "val_running_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "\n",
    "        classification_output, deflection_output = model(images)\n",
    "\n",
    "        class_loss = criterion(classification_output, labels)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        theta = torch.ones(batch_size, device=device)\n",
    "        beta = torch.zeros(batch_size, device=device)\n",
    "        physics_loss = lensing_loss(deflection_output, theta, beta, D_LS, D_S)\n",
    "\n",
    "        total_loss = class_loss + physics_loss_weight * physics_loss\n",
    "        val_running_loss += total_loss.item()\n",
    "\n",
    "print(f\"Final Validation Loss: {val_running_loss/len(valid_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-03CDexTkY9"
   },
   "source": [
    "**Save model weights to JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcU3iS5GHmrb",
    "outputId": "bbb24f5b-3903-4bf0-9a9f-066c819b2490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to lensing_resnet50_weights.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def save_model_weights_json(model, filename=\"lensing_resnet50_weights.json\"):\n",
    "\n",
    "    # Convert model weights to CPU and list format\n",
    "    state_dict = {k: v.cpu().tolist() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(state_dict, f)\n",
    "\n",
    "    print(f\"Model weights saved to {filename}\")\n",
    "\n",
    "# Save model weights\n",
    "save_model_weights_json(model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
